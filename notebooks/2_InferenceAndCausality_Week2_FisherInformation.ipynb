{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84cd6398",
   "metadata": {},
   "source": [
    "\n",
    "# Fisher Information — A Gentle, Practical Tour\n",
    "\n",
    "> *“Information is what bends the curve.”*  \n",
    "Think of Fisher Information as how **curved** your log-likelihood is around the true parameter. More curvature ⇒ more precision ⇒ less variance in your estimator.\n",
    "\n",
    "This notebook walks you through:\n",
    "- What Fisher Information is and why it shows up as the **curvature** of the log-likelihood.\n",
    "- The two equivalent definitions (squared score vs. negative expected Hessian).\n",
    "- **Observed** vs **Expected** information.\n",
    "- Classic, crunchy examples (Bernoulli, Normal).\n",
    "- The **Cramér–Rao Lower Bound (CRLB)** and how to read it.\n",
    "- A quick numerical demo (finite differences & Monte Carlo).\n",
    "- Short exercises you can run in class.\n",
    "\n",
    "The tone aims for *Think Bayes 2*: friendly, direct, a bit witty, and highly hands-on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc652d",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Why “curvature” means “information”\n",
    "\n",
    "Imagine the log-likelihood, $\\ell(\\theta) = \\log L(\\theta; x)$, as a hill over $\\theta$.  \n",
    "- The **peak** is your estimate.  \n",
    "- The **steepness** around the peak tells how tightly the data pin down $\\theta$.\n",
    "\n",
    "If the hill is **narrow and steep**, nudging $\\theta$ left or right drops the log-likelihood quickly.  \n",
    "Data are very *sensitive* to $\\theta$. That’s **high information**.\n",
    "\n",
    "If the hill is **flat and wide**, many $\\theta$ values fit almost equally well.  \n",
    "Data are not very sensitive to $\\theta$. That’s **low information**.\n",
    "\n",
    "Mathematically, **curvature** is the second derivative. Fisher Information takes the **expected curvature** of $\\ell(\\theta)$ (or, equivalently, the expected squared slope), averaged over datasets generated at the true $\\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a72230",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Two equivalent definitions\n",
    "\n",
    "Let $X \\sim f(x \\mid \\theta)$. Define the **score** as the derivative of the log-likelihood:\n",
    "$$\n",
    "S(\\theta; X) \\;=\\; \\frac{d}{d\\theta}\\,\\log f(X \\mid \\theta).\n",
    "$$\n",
    "\n",
    "Under standard regularity conditions, the **Fisher Information** is\n",
    "$$\n",
    "\\boxed{\\,I(\\theta) \\;=\\; \\mathbb{E}\\!\\left[S(\\theta; X)^2\\right] \\;=\\; -\\,\\mathbb{E}\\!\\left[\\frac{d^2}{d\\theta^2}\\log f(X \\mid \\theta)\\right]\\,}\n",
    "$$\n",
    "\n",
    "- The expectation is with respect to $X \\sim f(\\cdot \\mid \\theta)$.\n",
    "- The thing **inside** the expectation is sometimes called the **observed information** for the realized sample.\n",
    "- The expectation itself gives the **Fisher Information** (a property of the model at $\\theta$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea9306b",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Observed vs Expected Information\n",
    "\n",
    "- **Observed information** (for your actual dataset $x$) is the negative second derivative evaluated at $x$:\n",
    "$$\n",
    "J(\\theta; x) \\;=\\; -\\,\\frac{d^2}{d\\theta^2}\\log f(x \\mid \\theta).\n",
    "$$\n",
    "\n",
    "- **Fisher (expected) information** averages the observed information over all possible $x$ generated at $\\theta$:\n",
    "$$\n",
    "I(\\theta) \\;=\\; \\mathbb{E}\\big[J(\\theta; X)\\big].\n",
    "$$\n",
    "\n",
    "Think: *Observed* = what your current sample says. *Expected* = what the data-generating process says **on average**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45608133",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Cramér–Rao Lower Bound (CRLB)\n",
    "\n",
    "Fisher Information is the inverse of the best-possible variance—at least for unbiased estimators:\n",
    "$$\n",
    "\\mathrm{Var}(\\hat\\theta) \\;\\ge\\; \\frac{1}{I(\\theta)}.\n",
    "$$\n",
    "\n",
    "Big information $\\Rightarrow$ small variance.  \n",
    "Small information $\\Rightarrow$ variance can’t be pushed much lower.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20918d34",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Example: Bernoulli($p$)\n",
    "\n",
    "Let $X \\in \\{0,1\\}$ with $P(X=1)=p$. One observation has\n",
    "$$\n",
    "\\log f(x \\mid p) \\;=\\; x\\log p + (1-x)\\log(1-p).\n",
    "$$\n",
    "The score is\n",
    "$$\n",
    "\\frac{d}{dp}\\log f(x \\mid p) \\;=\\; \\frac{x}{p} - \\frac{1-x}{1-p}.\n",
    "$$\n",
    "Take the expectation over $X \\sim \\mathrm{Bernoulli}(p)$:\n",
    "$$\n",
    "I(p) \\;=\\; \\mathbb{E}\\!\\left[\\left(\\frac{x}{p} - \\frac{1-x}{1-p}\\right)^2\\right] \\;=\\; \\frac{1}{p(1-p)}.\n",
    "$$\n",
    "\n",
    "For $n$ IID observations, information **adds**:\n",
    "$$\n",
    "I_n(p) \\;=\\; n \\cdot \\frac{1}{p(1-p)}.\n",
    "$$\n",
    "\n",
    "Information is highest near $p=0.5$, and lowest near 0 or 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f7947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p = np.linspace(0.01, 0.99, 300)\n",
    "I = 1.0/(p*(1-p))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(p, I)\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"I(p)\")\n",
    "plt.title(\"Fisher Information for Bernoulli(p) — one observation\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e2d2f4",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Example: Normal mean with known variance\n",
    "\n",
    "Suppose $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ with **known** $\\sigma^2$, and $\\mu$ is the parameter.\n",
    "\n",
    "$$\n",
    "\\log f(x \\mid \\mu) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2}.\n",
    "$$\n",
    "Score:\n",
    "$$\n",
    "\\frac{d}{d\\mu}\\log f(x \\mid \\mu) = \\frac{x - \\mu}{\\sigma^2}.\n",
    "$$\n",
    "Fisher Information (one observation):\n",
    "$$\n",
    "I(\\mu) = \\mathbb{E}\\!\\left[\\left(\\frac{x - \\mu}{\\sigma^2}\\right)^2\\right] = \\frac{1}{\\sigma^2}.\n",
    "$$\n",
    "For $n$ IID observations: $I_n(\\mu) = \\dfrac{n}{\\sigma^2}$.\n",
    "\n",
    "CRLB says $\\mathrm{Var}(\\hat\\mu) \\ge \\sigma^2/n$.  \n",
    "The sample mean actually **achieves** this bound (it’s efficient).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9471f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Small numerical sanity check: as n grows, variance of the MLE ~ sigma^2/n\n",
    "rng = np.random.default_rng(42)\n",
    "mu_true = 2.0\n",
    "sigma = 3.0\n",
    "trials = 3000  # reduced a bit to keep execution snappy\n",
    "\n",
    "def simulate_var(n):\n",
    "    means = []\n",
    "    for _ in range(trials):\n",
    "        x = rng.normal(mu_true, sigma, size=n)\n",
    "        means.append(np.mean(x))\n",
    "    return np.var(means, ddof=1)\n",
    "\n",
    "ns = np.array([1, 2, 5, 10, 20, 50, 100])\n",
    "emp_var = np.array([simulate_var(int(n)) for n in ns])\n",
    "theory = (sigma**2)/ns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(ns, emp_var, marker=\"o\", label=\"Empirical Var(mean)\")\n",
    "plt.plot(ns, theory, marker=\"x\", label=\"CRLB σ²/n\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.title(\"Sample mean variance vs CRLB (Normal, σ known)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0164aca2",
   "metadata": {},
   "source": [
    "\n",
    "## 7. When you don’t have a closed form: numeric approximations\n",
    "\n",
    "Two handy strategies:\n",
    "\n",
    "1. **Monte Carlo** (expected squared score):  \n",
    "   - Draw $x^{(1)},\\dots,x^{(M)} \\sim f(\\cdot \\mid \\theta)$.  \n",
    "   - Compute the score $S(\\theta; x^{(m)})$.  \n",
    "   - Average $S^2$ across draws.\n",
    "\n",
    "2. **Hessian (curvature) at the MLE**:  \n",
    "   For large $n$, $-$ the Hessian of the log-likelihood at the MLE $\\hat\\theta$ approximates the **observed** information;  \n",
    "   its expectation approximates Fisher information.\n",
    "\n",
    "Below we do both for Bernoulli($p$), purely to show they agree with the closed-form $1/[p(1-p)]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def bernoulli_score(x, p):\n",
    "    return x/p - (1-x)/(1-p)\n",
    "\n",
    "def fisher_mc_bernoulli(p, M=150000, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    xs = rng.binomial(1, p, size=M)\n",
    "    s = bernoulli_score(xs, p)\n",
    "    return np.mean(s**2)\n",
    "\n",
    "for p in [0.2, 0.5, 0.8]:\n",
    "    est = fisher_mc_bernoulli(p, M=150000, rng=np.random.default_rng(0))\n",
    "    exact = 1/(p*(1-p))\n",
    "    print(f\"p={p:.1f}  MC≈{est:.5f}  exact={exact:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a870913",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def loglik_bernoulli(p, x):\n",
    "    p = np.clip(p, 1e-12, 1-1e-12)  # guardrails\n",
    "    return x*np.log(p) + (1-x)*np.log(1-p)\n",
    "\n",
    "def hessian_fd_bernoulli(p, x, h=1e-5):\n",
    "    # 1D second derivative via central differences\n",
    "    l1 = loglik_bernoulli(p+h, x)\n",
    "    l2 = loglik_bernoulli(p, x)\n",
    "    l0 = loglik_bernoulli(p-h, x)\n",
    "    second = (l1 - 2*l2 + l0)/(h**2)\n",
    "    return second  # second derivative of loglik\n",
    "\n",
    "# Observed information J = -d2/dp2 loglik\n",
    "rng = np.random.default_rng(123)\n",
    "p_true = 0.6\n",
    "n = 3000\n",
    "x = rng.binomial(1, p_true, size=n)\n",
    "# Sum log-likelihood across samples, Hessian adds\n",
    "second_sum = sum(hessian_fd_bernoulli(p_true, xi) for xi in x)\n",
    "J_obs = -second_sum\n",
    "I_theory = n/(p_true*(1-p_true))\n",
    "\n",
    "print(f\"Observed info (finite diff at true p) ≈ {J_obs:.3f}\")\n",
    "print(f\"Theoretical Fisher info            = {I_theory:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c991e",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Brief note: multivariate Fisher Information\n",
    "\n",
    "If $\\theta \\in \\mathbb{R}^k$, Fisher Information is a **matrix**:\n",
    "$$\n",
    "I(\\theta) \\;=\\; \\mathbb{E}\\!\\left[\\,\\nabla_\\theta \\log f(X\\mid\\theta)\\;\\nabla_\\theta \\log f(X\\mid\\theta)^\\top\\,\\right]\n",
    "\\;=\\; -\\,\\mathbb{E}\\!\\left[\\,\\nabla_\\theta^2 \\log f(X\\mid\\theta)\\,\\right].\n",
    "$$\n",
    "\n",
    "- Diagonals = information about each component (precision).  \n",
    "- Off-diagonals = how parameters *interact* (curvature coupling).\n",
    "\n",
    "In practice: invert $I(\\theta)$ (or its plug-in estimate) to approximate the **asymptotic covariance** of MLEs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9fb70b",
   "metadata": {},
   "source": [
    "## Excersice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9af315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given counts (n, k) from Bernoulli trials:\n",
    "# n is the number of trials, here 50\n",
    "# k is the number of successes, here 32\n",
    "# 1) Compute the MLE (p_hat) p̂ = k/n.\n",
    "# 2) Compute Fisher information for n trials: I_n(p̂) = n / [p̂ (1−p̂)].\n",
    "# Run once, read the printed values, then fill STUDENT_* and re-run to check.\n",
    "\n",
    "# Fill the following with the appropriate values instead of zeros:\n",
    "n, k = 0, 0\n",
    "p_hat = 0\n",
    "I_n   = 0\n",
    "\n",
    "print(f\"Counts: k={k}, n={n}\")\n",
    "print(f\"p_hat = {p_hat:.4f}\")\n",
    "print(f\"Fisher info I_n(p_hat) = {I_n:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa46130",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Takeaways\n",
    "\n",
    "- **Fisher Information = expected curvature** of the log-likelihood (or expected squared score).  \n",
    "- It quantifies how **sensitive** the data are to $\\theta$.  \n",
    "- High information ⇒ **low variance** lower bound (CRLB).  \n",
    "- For IID data, information usually **adds** linearly in $n$.  \n",
    "- When algebra is messy, **simulate** (Monte Carlo) or **differentiate numerically** (Hessian) to approximate it.\n",
    "\n",
    "If you keep only one mental picture: *steeper log-likelihood* = *more information* = *tighter estimates*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
